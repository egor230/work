import hashlib, logging, math, os, urllib.request, warnings, numpy as np, onnxruntime as rt, torch, torch.nn.functional as F, torchaudio, torchaudio.functional as taF, omegaconf
from abc import ABC, abstractmethod
from pathlib import Path
from subprocess import CalledProcessError, run
from typing import Dict, List, Optional, Tuple, Union
from torch import Tensor, nn
from torch.jit import TracerWarning
from sentencepiece import SentencePieceProcessor
from tqdm import tqdm
from pyannote.audio import Model, Pipeline
from pyannote.audio.core.task import Problem, Resolution, Specifications
from pyannote.audio.pipelines import VoiceActivityDetection

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
SAMPLE_RATE = 16000
LONGFORM_THRESHOLD = 35 * SAMPLE_RATE  # 25 —Å–µ–∫—É–Ω–¥ –ø–æ—Ä–æ–≥ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∞—É–¥–∏–æ
DTYPE = np.float32
MAX_LETTERS_PER_FRAME = 3
_URL_DIR = "https://cdn.chatwm.opensmodel.sberdevices.ru/GigaAM"
_MODEL_HASHES = {
 "emo": "7ce76f9535cb254488985057c0d33006",
 "v1_ctc": "f027f199e590a391d015aeede2e66174",
 "v1_rnnt": "02c758999bcdc6afcb2087ef256d47ef",
 "v1_ssl": "dc7f7b231f7f91c4968dc21910e7b396",
 "v2_ctc": "e00f59cb5d39624fb30d1786044795bf",
 "v2_rnnt": "547460139acfebd842323f59ed54ab54",
 "v2_ssl": "cd4cf819c8191a07b9d7edcad111668e",
 "v3_ctc": "73413e7be9c6a5935827bfab5c0dd678",
 "v3_rnnt": "0fd2c9a1ff66abd8d32a3a07f7592815",
 "v3_e2e_ctc": "367074d6498f426d960b25f49531cf68",
 "v3_e2e_rnnt": "2730de7545ac43ad256485a462b0a27a",
 "v3_ssl": "70cbf5ed7303a0ed242ddb257e9dc6a6",
}

_PIPELINE = None  # –ö—ç—à –¥–ª—è VAD –ø–∞–π–ø–ª–∞–π–Ω–∞
warnings.simplefilter("ignore", category=UserWarning)  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è


def load_audio(audio_input: Union[str, np.ndarray, Tensor], sample_rate: int = SAMPLE_RATE) -> Tensor:
 """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ –∏–∑ —Ñ–∞–π–ª–∞, –º–∞—Å—Å–∏–≤–∞ –∏–ª–∏ —Ç–µ–Ω–∑–æ—Ä–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–æ 16–∫–ì—Ü"""
 if isinstance(audio_input, str):
  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ffmpeg –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤
  cmd = ["ffmpeg", "-nostdin", "-threads", "0", "-i", audio_input, "-f", "s16le", "-ac", "1", "-acodec", "pcm_s16le", "-ar", str(sample_rate), "-"]
  try:
   audio_bytes = run(cmd, capture_output=True, check=True).stdout
  except CalledProcessError as exc:
   raise RuntimeError("Failed to load audio") from exc
  with warnings.catch_warnings():
   warnings.simplefilter("ignore", category=UserWarning)
   audio_tensor = torch.frombuffer(audio_bytes, dtype=torch.int16).float() / 32768.0
 elif isinstance(audio_input, np.ndarray):
  audio_tensor = torch.from_numpy(audio_input.copy()).float()
 elif isinstance(audio_input, Tensor):
  audio_tensor = audio_input.float().clone()
 else:
  raise TypeError(f"Unsupported audio input type: {type(audio_input)}. Expected str, np.ndarray, or Tensor.")

 # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫ –æ–¥–Ω–æ–∫–∞–Ω–∞–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
 if audio_tensor.ndim > 1:
  audio_tensor = audio_tensor.flatten()

 # –†–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º –¥–æ —Ü–µ–ª–µ–≤–æ–π —á–∞—Å—Ç–æ—Ç—ã –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
 if sample_rate != SAMPLE_RATE:
  if audio_tensor.numel() > 0:
   audio_tensor = taF.resample(audio_tensor.unsqueeze(0), orig_freq=sample_rate, new_freq=SAMPLE_RATE).squeeze(0)

 return audio_tensor


class SpecScaler(nn.Module):
 """–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""

 def forward(self, x: Tensor) -> Tensor:
  return torch.log(x.clamp_(1e-9, 1e9))  # clip –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏


class FeatureExtractor(nn.Module):
 """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ –∞—É–¥–∏–æ"""

 def __init__(self, sample_rate: int, features: int, hop_length=None, win_length=None, n_fft=None, center=None, **kwargs):
  super().__init__()
  self.hop_length = hop_length if hop_length is not None else kwargs.get("hop_length", sample_rate // 100)
  self.win_length = win_length if win_length is not None else kwargs.get("win_length", sample_rate // 40)
  self.n_fft = n_fft if n_fft is not None else kwargs.get("n_fft", sample_rate // 40)
  self.center = center if center is not None else kwargs.get("center", True)

  # –ú–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ + –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ
  self.featurizer = nn.Sequential(
   torchaudio.transforms.MelSpectrogram(
    sample_rate=sample_rate,
    n_mels=features,
    win_length=self.win_length,
    hop_length=self.hop_length,
    n_fft=self.n_fft,
    center=self.center
   ),
   SpecScaler()
  )

 def out_len(self, input_lengths: Tensor) -> Tensor:
  """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–ª–∏–Ω—É –≤—ã—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
  if self.center:
   return input_lengths.div(self.hop_length, rounding_mode="floor").add(1).long()
  else:
   return (input_lengths - self.win_length).div(self.hop_length, rounding_mode="floor").add(1).long()

 def forward(self, input_signal: Tensor, length: Tensor) -> Tuple[Tensor, Tensor]:
  return self.featurizer(input_signal), self.out_len(length)


class CTCHead(nn.Module):
 """–ì–æ–ª–æ–≤–∞ CTC –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏"""

 def __init__(self, feat_in: int, num_classes: int):
  super().__init__()
  self.decoder_layers = torch.nn.Sequential(
   torch.nn.Conv1d(feat_in, num_classes, kernel_size=1)
  )

 def forward(self, encoder_output: Tensor) -> Tensor:
  return torch.nn.functional.log_softmax(
   self.decoder_layers(encoder_output).transpose(1, 2),
   dim=-1
  )


class RNNTJoint(nn.Module):
 """Joint —Å–µ—Ç—å –¥–ª—è RNN-T –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""

 def __init__(self, enc_hidden: int, pred_hidden: int, joint_hidden: int, num_classes: int):
  super().__init__()
  self.enc_hidden = enc_hidden
  self.pred_hidden = pred_hidden
  self.pred = nn.Linear(pred_hidden, joint_hidden)
  self.enc = nn.Linear(enc_hidden, joint_hidden)
  self.joint_net = nn.Sequential(
   nn.ReLU(),
   nn.Linear(joint_hidden, num_classes)
  )

 def joint(self, encoder_out: Tensor, decoder_out: Tensor) -> Tensor:
  """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—ã—Ö–æ–¥—ã —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞"""
  enc = self.enc(encoder_out).unsqueeze(2)
  pred = self.pred(decoder_out).unsqueeze(1)
  return self.joint_net(enc + pred).log_softmax(-1)

 def forward(self, enc: Tensor, dec: Tensor) -> Tensor:
  return self.joint(enc.transpose(1, 2), dec.transpose(1, 2))


class RNNTDecoder(nn.Module):
 """–î–µ–∫–æ–¥–µ—Ä –¥–ª—è RNN-T –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""

 def __init__(self, pred_hidden: int, pred_rnn_layers: int, num_classes: int):
  super().__init__()
  self.blank_id = num_classes - 1  # ID –¥–ª—è blank —Å–∏–º–≤–æ–ª–∞
  self.pred_hidden = pred_hidden
  self.embed = nn.Embedding(num_classes, pred_hidden, padding_idx=self.blank_id)
  self.lstm = nn.LSTM(pred_hidden, pred_hidden, pred_rnn_layers)

 def predict(self, x: Optional[Tensor], state: Optional[Tensor], batch_size: int = 1) -> Tuple[Tensor, Tensor]:
  """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–∏–º–≤–æ–ª–∞"""
  if x is not None:
   emb: Tensor = self.embed(x)
  else:
   emb = torch.zeros((batch_size, 1, self.pred_hidden), device=next(self.parameters()).device)
  g, hid = self.lstm(emb.transpose(0, 1), state)
  return g.transpose(0, 1), hid

 def forward(self, x: Tensor, h: Tensor, c: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
  emb = self.embed(x)
  g, (h, c) = self.lstm(emb.transpose(0, 1), (h, c))
  return g.transpose(0, 1), h, c


class RNNTHead(nn.Module):
 """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–µ–∫–æ–¥–µ—Ä –∏ joint —Å–µ—Ç—å RNN-T"""

 def __init__(self, decoder: Dict[str, int], joint: Dict[str, int]):
  super().__init__()
  self.decoder = RNNTDecoder(**decoder)
  self.joint = RNNTJoint(**joint)


class Tokenizer:
 """–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ ID –∏ –æ–±—Ä–∞—Ç–Ω–æ"""

 def __init__(self, vocab: List[str], model_path: Optional[str] = None):
  self.charwise = model_path is None  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏ character-level —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
  if self.charwise:
   self.vocab = vocab
  else:
   self.model = SentencePieceProcessor()
   self.model.load(model_path)

 def decode(self, tokens: List[int]) -> str:
  if self.charwise:
   return "".join(self.vocab[tok] for tok in tokens)
  return self.model.decode(tokens)

 def __len__(self):
  return len(self.vocab) if self.charwise else len(self.model)


class CTCGreedyDecoding:
 """–ñ–∞–¥–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è CTC –º–æ–¥–µ–ª–µ–π"""

 def __init__(self, vocabulary: List[str], model_path: Optional[str] = None):
  self.tokenizer = Tokenizer(vocabulary, model_path)
  self.blank_id = len(self.tokenizer)  # ID –¥–ª—è blank —Å–∏–º–≤–æ–ª–∞

 @torch.inference_mode()
 def decode(self, head: CTCHead, encoded: Tensor, lengths: Tensor) -> List[str]:
  log_probs = head(encoder_output=encoded)
  b, _, c = log_probs.shape

  # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
  labels = log_probs.argmax(dim=-1, keepdim=False)

  # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã –∏ blank
  skip_mask = labels != self.blank_id
  skip_mask[:, 1:] = torch.logical_and(skip_mask[:, 1:], labels[:, 1:] != labels[:, :-1])

  # –û–±—Ä–µ–∑–∞–µ–º –ø–æ –¥–ª–∏–Ω–µ
  for i, length in enumerate(lengths):
   skip_mask[i, length:] = 0

  # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç
  pred_texts: List[str] = []
  for i in range(b):
   pred_texts.append("".join(self.tokenizer.decode(labels[i][skip_mask[i]].cpu().tolist())))

  return pred_texts

class RNNTGreedyDecoding:
 """–ñ–∞–¥–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è RNN-T –º–æ–¥–µ–ª–µ–π"""

 def __init__(self, vocabulary: List[str], model_path: Optional[str] = None, max_symbols_per_step: int = 30):
  self.tokenizer = Tokenizer(vocabulary, model_path)# –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10
  self.blank_id = len(self.tokenizer)
  self.max_symbols = 50  # –ú–∞–∫—Å. —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —à–∞–≥

 def _greedy_decode(self, head: RNNTHead, x: Tensor, seqlen: Tensor) -> str:
  hyp: List[int] = []  # –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö ID
  dec_state: Optional[Tensor] = None  # –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–µ–∫–æ–¥–µ—Ä–∞
  last_label: Optional[Tensor] = None  # –ü–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª

  for t in range(seqlen):
   f = x[t, :, :].unsqueeze(1)  # –¢–µ–∫—É—â–∏–π –∫–∞–¥—Ä –∞—É–¥–∏–æ
   not_blank = True
   new_symbols = 0

   # –ü–æ–∫–∞ –Ω–µ –≤—Å—Ç—Ä–µ—Ç–∏–º blank –∏ –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤
   while not_blank and new_symbols < self.max_symbols:
    g, hidden = head.decoder.predict(last_label, dec_state)
    joint_out = head.joint.joint(f, g)
    k = joint_out[0, 0, 0, :].argmax(0).item()  # ID —Å–∏–º–≤–æ–ª–∞
    max_prob = joint_out[0, 0, 0, :].max().exp()  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ blank –ª–∏ —ç—Ç–æ –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    if k == self.blank_id or max_prob.item() < 0.4: # —ç—Ç–æ –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Å–ª–æ–≤–µ
     not_blank = False
    else:
     hyp.append(int(k))
     dec_state = hidden
     last_label = torch.tensor([[hyp[-1]]]).to(x.device)
     new_symbols += 1

  return self.tokenizer.decode(hyp)

 @torch.inference_mode()
 def decode(self, head: RNNTHead, encoded: Tensor, enc_len: Tensor) -> List[str]:
  b = encoded.shape[0]
  pred_texts = []
  encoded = encoded.transpose(1, 2)  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω—É–∂–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É

  # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞
  for i in range(b):
   inseq = encoded[i, :, :].unsqueeze(1)
   pred_texts.append(self._greedy_decode(head, inseq, enc_len[i]))

  return pred_texts


class StridingSubsampling(nn.Module):
 """–°—É–±—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–≤–µ—Ä—Ç–æ–∫ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""

 def __init__(self, subsampling: str, kernel_size: int, subsampling_factor: int,
              feat_in: int, feat_out: int, conv_channels: int):
  super().__init__()
  self.subsampling_type = subsampling
  assert self.subsampling_type in ["conv1d", "conv2d"]

  self._sampling_num = int(math.log(subsampling_factor, 2))
  self._stride = 2
  self._kernel_size = kernel_size
  self._padding = (self._kernel_size - 1) // 2

  layers: List[nn.Module] = []
  in_channels = 1 if self.subsampling_type == "conv2d" else feat_in
  subs_conv_class = torch.nn.Conv2d if self.subsampling_type == "conv2d" else torch.nn.Conv1d

  for _ in range(self._sampling_num):
   layers.append(subs_conv_class(  in_channels=in_channels,    out_channels=conv_channels,
    kernel_size=self._kernel_size,
    stride=self._stride,   padding=self._padding
   ))
   layers.append(nn.ReLU())
   in_channels = conv_channels

  if self.subsampling_type == "conv2d":
   out_length = self.calc_output_length(torch.tensor(feat_in))
   self.out = torch.nn.Linear(conv_channels * int(out_length), feat_out)

  self.conv = torch.nn.Sequential(*layers)

 def calc_output_length(self, lengths: Tensor) -> Tensor:
  """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–ª–∏–Ω—É –≤—ã—Ö–æ–¥–∞ –ø–æ—Å–ª–µ —Å—É–±—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è"""
  lengths = lengths.to(torch.float)
  add_pad = 2 * self._padding - self._kernel_size

  for _ in range(self._sampling_num):
   lengths = torch.div(lengths + add_pad, self._stride) + 1.0
   lengths = torch.floor(lengths)

  return lengths.to(dtype=torch.int)

 def forward(self, x: Tensor, lengths: Tensor) -> Tuple[Tensor, Tensor]:
  if self.subsampling_type == "conv2d":
   x = self.conv(x.unsqueeze(1))
   b, _, t, _ = x.size()
   x = self.out(x.transpose(1, 2).reshape(b, t, -1))
  else:
   x = self.conv(x.transpose(1, 2)).transpose(1, 2)

  return x, self.calc_output_length(lengths)

class MultiHeadAttention(nn.Module, ABC):
 """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è"""

 def __init__(self, n_head: int, n_feat: int):
  super().__init__()
  assert n_feat % n_head == 0
  self.d_k = n_feat // n_head  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ head
  self.h = n_head

  # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è Q, K, V
  self.linear_q = nn.Linear(n_feat, n_feat)
  self.linear_k = nn.Linear(n_feat, n_feat)
  self.linear_v = nn.Linear(n_feat, n_feat)
  self.linear_out = nn.Linear(n_feat, n_feat)

 def forward_qkv(self, query: Tensor, key: Tensor, value: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
  b = query.size(0)
  q = self.linear_q(query).view(b, -1, self.h, self.d_k)
  k = self.linear_k(key).view(b, -1, self.h, self.d_k)
  v = self.linear_v(value).view(b, -1, self.h, self.d_k)

  return q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)

 def forward_attention(self, value: Tensor, scores: Tensor, mask: Optional[Tensor]) -> Tensor:
  b = value.size(0)

  if mask is not None:
   mask = mask.unsqueeze(1)
   scores = scores.masked_fill(mask, -10000.0)
   attn = torch.softmax(scores, dim=-1).masked_fill(mask, 0.0)
  else:
   attn = torch.softmax(scores, dim=-1)

  x = torch.matmul(attn, value)
  x = x.transpose(1, 2).reshape(b, -1, self.h * self.d_k)

  return self.linear_out(x)


class RelPositionMultiHeadAttention(MultiHeadAttention):
 """Attention —Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""

 def __init__(self, n_head: int, n_feat: int):
  super().__init__(n_head, n_feat)
  self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)
  self.pos_bias_u = nn.Parameter(torch.FloatTensor(self.h, self.d_k))
  self.pos_bias_v = nn.Parameter(torch.FloatTensor(self.h, self.d_k))

 def rel_shift(self, x: Tensor) -> Tensor:
  """–°–¥–≤–∏–≥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
  b, h, qlen, pos_len = x.size()
  x = torch.nn.functional.pad(x, pad=(1, 0))
  x = x.view(b, h, -1, qlen)
  return x[:, :, 1:].view(b, h, qlen, pos_len)

 def forward(self, query: Tensor, key: Tensor, value: Tensor, pos_emb: Tensor, mask: Optional[Tensor] = None) -> Tensor:
  q, k, v = self.forward_qkv(query, key, value)
  q = q.transpose(1, 2)

  # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
  p = self.linear_pos(pos_emb)
  p = p.view(pos_emb.shape[0], -1, self.h, self.d_k).transpose(1, 2)

  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è
  q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)
  q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)

  # –í—ã—á–∏—Å–ª—è–µ–º attention scores
  matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))
  matrix_bd = self.rel_shift(matrix_bd)
  matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))
  matrix_bd = matrix_bd[:, :, :, : matrix_ac.size(-1)]

  scores = (matrix_ac + matrix_bd) / math.sqrt(self.d_k)

  return self.forward_attention(v, scores, mask)


class RotaryPositionMultiHeadAttention(MultiHeadAttention):
 """Attention —Å rotary –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""

 def forward(self, query: Tensor, key: Tensor, value: Tensor, pos_emb: List[Tensor], mask: Optional[Tensor] = None) -> Tensor:
  b, t, _ = value.size()

  # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º Q, K, V
  query = query.transpose(0, 1).view(t, b, self.h, self.d_k)
  key = key.transpose(0, 1).view(t, b, self.h, self.d_k)
  value = value.transpose(0, 1).view(t, b, self.h, self.d_k)

  # –ü—Ä–∏–º–µ–Ω—è–µ–º rotary –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
  cos, sin = pos_emb
  query, key = apply_rotary_pos_emb(query, key, cos, sin, offset=0)

  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ
  q, k, v = self.forward_qkv(
   query.view(t, b, self.h * self.d_k).transpose(0, 1),
   key.view(t, b, self.h * self.d_k).transpose(0, 1),
   value.view(t, b, self.h * self.d_k).transpose(0, 1)
  )

  # –í—ã—á–∏—Å–ª—è–µ–º attention scores
  scores = torch.matmul(q, k.transpose(-2, -1) / math.sqrt(self.d_k))

  return self.forward_attention(v, scores, mask)


class PositionalEncoding(nn.Module, ABC):
 """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è"""

 def __init__(self, dim: int, base: int):
  super().__init__()
  self.dim = dim
  self.base = base

 @abstractmethod
 def create_pe(self, length: int, device: torch.device) -> Optional[Tensor]:
  pass

 def extend_pe(self, length: int, device: torch.device):
  """–†–∞—Å—à–∏—Ä—è–µ—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
  pe = self.create_pe(length, device)
  if pe is None:
   return
  if hasattr(self, "pe"):
   self.pe = pe
  else:
   self.register_buffer("pe", pe, persistent=False)


class RelPositionalEmbedding(PositionalEncoding):
 """–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""

 def create_pe(self, length: int, device: torch.device) -> Optional[Tensor]:
  if hasattr(self, "pe") and self.pe.shape[1] >= 2 * length - 1:
   return None

  positions = torch.arange(length - 1, -length, -1, device=device).unsqueeze(1)
  pos_length = positions.size(0)
  pe = torch.zeros(pos_length, self.dim, device=positions.device)

  # –°–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
  div_term = torch.exp(torch.arange(0, self.dim, 2, device=pe.device) * -(math.log(10000.0) / self.dim))
  pe[:, 0::2] = torch.sin(positions * div_term)
  pe[:, 1::2] = torch.cos(positions * div_term)

  return pe.unsqueeze(0)

 def forward(self, x: torch.Tensor) -> Tuple[Tensor, Tensor]:
  input_len = x.size(1)
  center_pos = self.pe.size(1) // 2 + 1
  start_pos = center_pos - input_len
  end_pos = center_pos + input_len - 1

  return x, self.pe[:, start_pos:end_pos]


class RotaryPositionalEmbedding(PositionalEncoding):
 """Rotary –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""

 def create_pe(self, length: int, device: torch.device) -> Optional[Tensor]:
  if hasattr(self, "pe") and self.pe.size(0) >= 2 * length:
   return None

  positions = torch.arange(0, length, dtype=torch.float32, device=device)
  inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))
  t = torch.arange(length, device=positions.device).type_as(inv_freq)

  freqs = torch.einsum("i,j->ij", t, inv_freq)
  emb = torch.cat((freqs, freqs), dim=-1).to(positions.device)

  return torch.cat([emb.cos()[:, None, None, :], emb.sin()[:, None, None, :]])

 def forward(self, x: torch.Tensor) -> Tuple[Tensor, List[Tensor]]:
  cos_emb = self.pe[0: x.shape[1]]
  half_pe = self.pe.shape[0] // 2
  sin_emb = self.pe[half_pe: half_pe + x.shape[1]]

  return x, [cos_emb, sin_emb]


class ConformerConvolution(nn.Module):
 """–°–≤–µ—Ä—Ç–æ—á–Ω—ã–π –º–æ–¥—É–ª—å Conformer"""

 def __init__(self, d_model: int, kernel_size: int, norm_type: str):
  super().__init__()
  assert (kernel_size - 1) % 2 == 0
  assert norm_type in ["batch_norm", "layer_norm"]

  self.norm_type = norm_type
  self.pointwise_conv1 = nn.Conv1d(d_model, d_model * 2, kernel_size=1)
  self.depthwise_conv = nn.Conv1d(   in_channels=d_model,   out_channels=d_model,
   kernel_size=kernel_size,   padding=(kernel_size - 1) // 2,   groups=d_model,  bias=True  )
  self.batch_norm = nn.BatchNorm1d(d_model) if norm_type == "batch_norm" else nn.LayerNorm(d_model)
  self.activation = nn.SiLU()
  self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)

 def forward(self, x: Tensor, pad_mask: Optional[Tensor] = None) -> Tensor:
  x = x.transpose(1, 2)
  x = self.pointwise_conv1(x)
  x = nn.functional.glu(x, dim=1)  # Gated Linear Unit

  if pad_mask is not None:
   x = x.masked_fill(pad_mask.unsqueeze(1), 0.0)

  x = self.depthwise_conv(x)

  if self.norm_type == "batch_norm":
   x = self.batch_norm(x)
  else:
   x = self.batch_norm(x.transpose(1, 2)).transpose(1, 2)

  x = self.activation(x)
  x = self.pointwise_conv2(x)

  return x.transpose(1, 2)


class ConformerFeedForward(nn.Module):
 """FeedForward —Å–ª–æ–π Conformer"""

 def __init__(self, d_model: int, d_ff: int, use_bias=True):
  super().__init__()
  self.linear1 = nn.Linear(d_model, d_ff, bias=use_bias)
  self.activation = nn.SiLU()
  self.linear2 = nn.Linear(d_ff, d_model, bias=use_bias)

 def forward(self, x: Tensor) -> Tensor:
  return self.linear2(self.activation(self.linear1(x)))


class ConformerLayer(nn.Module):
 """–û–¥–∏–Ω —Å–ª–æ–π Conformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""

 def __init__(self, d_model: int, d_ff: int, self_attention_model: str,
              n_heads: int = 16, conv_norm_type: str = "batch_norm",
              conv_kernel_size: int = 31):
  super().__init__()
  self.fc_factor = 0.5

  # FeedForward 1
  self.norm_feed_forward1 = nn.LayerNorm(d_model)
  self.feed_forward1 = ConformerFeedForward(d_model=d_model, d_ff=d_ff)

  # Self-Attention
  self.norm_self_att = nn.LayerNorm(d_model)
  if self_attention_model == "rotary":
   self.self_attn: nn.Module = RotaryPositionMultiHeadAttention(n_head=n_heads, n_feat=d_model)
  else:
   self.self_attn = RelPositionMultiHeadAttention(n_head=n_heads, n_feat=d_model)

  # Convolution
  self.norm_conv = nn.LayerNorm(d_model)
  self.conv = ConformerConvolution(d_model=d_model, kernel_size=conv_kernel_size, norm_type=conv_norm_type)

  # FeedForward 2
  self.norm_feed_forward2 = nn.LayerNorm(d_model)
  self.feed_forward2 = ConformerFeedForward(d_model=d_model, d_ff=d_ff)

  # Output
  self.norm_out = nn.LayerNorm(d_model)

 def forward(self, x: Tensor, pos_emb: Union[Tensor, List[Tensor]],
             att_mask: Optional[Tensor] = None, pad_mask: Optional[Tensor] = None) -> Tensor:
  residual = x

  # FeedForward 1
  x = self.norm_feed_forward1(x)
  x = self.feed_forward1(x)
  residual = residual + x * self.fc_factor

  # Self-Attention
  x = self.norm_self_att(residual)
  x = self.self_attn(x, x, x, pos_emb, mask=att_mask)
  residual = residual + x

  # Convolution
  x = self.norm_conv(residual)
  x = self.conv(x, pad_mask=pad_mask)
  residual = residual + x

  # FeedForward 2
  x = self.norm_feed_forward2(residual)
  x = self.feed_forward2(x)
  residual = residual + x * self.fc_factor

  # Output
  x = self.norm_out(residual)

  return x


class ConformerEncoder(nn.Module):
 """Conformer —ç–Ω–∫–æ–¥–µ—Ä"""

 def __init__(self, feat_in: int = 64, n_layers: int = 16, d_model: int = 768,
              subsampling_factor: int = 4, ff_expansion_factor: int = 4,
              self_attention_model: str = "rotary", n_heads: int = 16,
              pos_emb_max_len: int = 5000, conv_kernel_size: int = 31,
              subsampling=None, subs_kernel_size=None, conv_norm_type=None, **kwargs):
  super().__init__()
  self.feat_in = feat_in
  assert self_attention_model in ["rotary", "rel_pos"], f"Not supported attn = {self_attention_model}"

  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
  subsampling = subsampling if subsampling is not None else kwargs.get("subsampling", "conv2d")
  subs_kernel_size = subs_kernel_size if subs_kernel_size is not None else kwargs.get("subs_kernel_size", 3)
  conv_norm_type = conv_norm_type if conv_norm_type is not None else kwargs.get("conv_norm_type", "batch_norm")

  # –°—É–±—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
  self.pre_encode = StridingSubsampling( subsampling=subsampling,
   kernel_size=subs_kernel_size, subsampling_factor=subsampling_factor,
   feat_in=feat_in, feat_out=d_model,  conv_channels=d_model  )

  self.pos_emb_max_len = pos_emb_max_len

  # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
  if self_attention_model == "rotary":
   self.pos_enc: PositionalEncoding = RotaryPositionalEmbedding(d_model // n_heads, pos_emb_max_len)
  else:
   self.pos_enc = RelPositionalEmbedding(d_model, pos_emb_max_len)

  # –°–ª–æ–∏ Conformer
  self.layers = nn.ModuleList()
  for _ in range(n_layers):
   layer = ConformerLayer(  d_model=d_model,  d_ff=d_model * ff_expansion_factor,
    self_attention_model=self_attention_model,
    n_heads=n_heads,  conv_norm_type=conv_norm_type,  conv_kernel_size=conv_kernel_size  )
   self.layers.append(layer)

 def forward(self, audio_signal: Tensor, length: Tensor) -> Tuple[Tensor, Tensor]:
  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
  if not hasattr(self.pos_enc, "pe"):
   self.pos_enc.extend_pe(self.pos_emb_max_len, audio_signal.device)

  # –°—É–±—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
  audio_signal, length = self.pre_encode(x=audio_signal.transpose(1, 2), lengths=length)
  max_len = audio_signal.size(1)

  # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
  audio_signal, pos_emb = self.pos_enc(x=audio_signal)

  # –ú–∞—Å–∫–∏
  pad_mask = torch.arange(0, max_len, device=audio_signal.device).expand(length.size(0), -1) < length.unsqueeze(-1)
  att_mask = None

  if audio_signal.shape[0] > 1:
   att_mask = pad_mask.unsqueeze(1).repeat([1, max_len, 1])
   att_mask = torch.logical_and(att_mask, att_mask.transpose(1, 2))
   att_mask = ~att_mask

  pad_mask = ~pad_mask

  # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏
  for layer in self.layers:
   audio_signal = layer(x=audio_signal, pos_emb=pos_emb, att_mask=att_mask, pad_mask=pad_mask)

  return audio_signal.transpose(1, 2), length

def get_pipeline() -> Pipeline:
 """–°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π VAD –ø–∞–π–ø–ª–∞–π–Ω"""
 global _PIPELINE
 if _PIPELINE is not None:
  return _PIPELINE

 # –¢—Ä–µ–±—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω HuggingFace
 try:
  hf_token = os.environ["HF_TOKEN"]
 except KeyError as exc:
  raise ValueError("HF_TOKEN environment variable is not set") from exc

 # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
 with torch.serialization.safe_globals([TorchVersion, Problem, Specifications, Resolution]):
  model = Model.from_pretrained("pyannote/segmentation-3.0", token=hf_token)

 _PIPELINE = VoiceActivityDetection(segmentation=model)
 _PIPELINE.instantiate({"min_duration_on": 0.0, "min_duration_off": 0.0})

 return _PIPELINE
 '''
    def segment_audio_file(wav_input, sr, 
                           max_duration: float = 22.0,  # ‚Üê –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
                           min_duration: float = 15.0,  # ‚Üê –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                           strict_limit_duration: float = 30.0,  # ‚Üê –ª–∏–º–∏—Ç –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
                           new_chunk_threshold: float = 0.2):  # ‚Üê –ø–æ—Ä–æ–≥ –¥–ª—è –Ω–æ–≤–æ–≥–æ —á–∞–Ω–∫–∞
    –ß—Ç–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å:
    new_chunk_threshold - —É–≤–µ–ª–∏—á–∏—Ç—å —Å 0.2 –¥–æ 0.5-1.0 —Å–µ–∫—É–Ω–¥—ã
    max_duration - —É–º–µ–Ω—å—à–∏—Ç—å —Å 22.0 –¥–æ 15.0-18.0 —Å–µ–∫—É–Ω–¥
    –î–ª—è —Ç–≤–æ–µ–π —Ä–µ—á–∏:  –ü–∞—É–∑—ã –º–æ–≥—É—Ç –±—ã—Ç—å –¥–ª–∏–Ω–Ω–µ–µ –æ–±—ã—á–Ω—ã—Ö ‚Üí
    —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π min_duration –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å —Ç–∞—â–∏—Ç—å —Å–µ–≥–º–µ–Ω—Ç –¥–∞–ª—å—à–µ, –¥–∞–∂–µ –∫–æ–≥–¥–∞ –∫–∞—á–µ—Å—Ç–≤–æ —É–∂–µ –ø–∞–¥–∞–µ—Ç.
    
    üü¢ –û–ø—Ç–∏–º—É–º: 6‚Äì8 —Å–µ–∫—É–Ω–¥    3Ô∏è‚É£ strict_limit_duration
    strict_limit_duration: float = 60.0        
    –ß—Ç–æ —ç—Ç–æ:   –ñ—ë—Å—Ç–∫–∏–π –ø–æ—Ç–æ–ª–æ–∫.
    –ï—Å–ª–∏ VAD ‚Äú–∑–∞–ª–∏–ø‚Äù –∏ –Ω–µ –≤–∏–¥–∏—Ç –ø–∞—É–∑ ‚Äî —Å–µ–≥–º–µ–Ω—Ç –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Ä–µ–∂–µ—Ç—Å—è.
    
    üî¥ 60 —Å–µ–∫—É–Ω–¥ ‚Äî —Å–ª–∏—à–∫–æ–º –æ–ø–∞—Å–Ω–æ   üü¢ –û–ø—Ç–∏–º—É–º: 20‚Äì25 —Å–µ–∫—É–Ω–¥
    
    4Ô∏è‚É£ new_chunk_threshold    new_chunk_threshold: float = 0.6     
    –°–∞–º—ã–π –≤–∞–∂–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è —Ç–µ–±—è.    –ß—Ç–æ —ç—Ç–æ:
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö), –∫–æ—Ç–æ—Ä–∞—è —Å—á–∏—Ç–∞–µ—Ç—Å—è ‚Äú–Ω–∞—Å—Ç–æ—è—â–µ–π‚Äù.
     –º–µ–Ω—å—à–µ ‚Üí –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è    –±–æ–ª—å—à–µ ‚Üí –º–æ–∂–Ω–æ –∑–∞–∫—Ä—ã–≤–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç
    
    –ü—Ä–∏ –î–¶–ü:    –ø–∞—É–∑—ã —á–∞—Å—Ç–æ –Ω–µ—Ä–æ–≤–Ω—ã–µ
    
    –µ—Å—Ç—å –º–∏–∫—Ä–æ-–æ—Å—Ç–∞–Ω–æ–≤–∫–∏    –¥—ã—Ö–∞–Ω–∏–µ –º–æ–∂–µ—Ç —Å–±–∏–≤–∞—Ç—å VAD
    
    üî¥ 0.2‚Äì0.3 ‚Üí –º–æ–¥–µ–ª—å —Ä–µ–∂–µ—Ç —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ
    üü¢ 0.8‚Äì1.2 ‚Üí –º–æ–¥–µ–ª—å —Ç–µ—Ä–ø–µ–ª–∏–≤–µ–µ, –Ω–æ –Ω–µ —Ç—è–Ω–µ—Ç –¥–æ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
    
    üîß –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–º–µ–Ω–Ω–æ –¥–ª—è —Ç–µ–±—è
    ‚≠ê –ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–†–ï–ö–û–ú–ï–ù–î–£–Æ)
segments, boundaries = segment_audio_file(
    audio_data,
    sr=SAMPLE_RATE,
    max_duration=16.0,          # –∫–ª—é—á–µ–≤–æ–µ: –Ω–µ –¥–∞—ë–º –º–æ–¥–µ–ª–∏ "—É—Å—Ç–∞–≤–∞—Ç—å"
    min_duration=7.0,           # –ø—Ä–∏–Ω–∏–º–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã
    strict_limit_duration=22.0, # —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ –æ—Ç –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
    new_chunk_threshold=1.0     # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –º–∏–∫—Ä–æ–ø–∞—É–∑—ã
)

üß† –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç CPU / RAM)
segments, boundaries = segment_audio_file(
    audio_data,
    sr=SAMPLE_RATE,
    max_duration=14.0,
    min_duration=6.0,
    strict_limit_duration=18.0,
    new_chunk_threshold=1.2
)
'''
 #–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç CPU / RAM)

def segment_audio_file(wav_input: Union[np.ndarray, Tensor], sr: int,
                       max_duration: float = 10.0, min_duration: float = 50.0,
                       strict_limit_duration: float = 20.0, new_chunk_threshold: float = 1.0) -> Tuple[List[torch.Tensor], List[Tuple[float, float]]]:
 """–°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ –≥–æ–ª–æ—Å–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
 if isinstance(wav_input, np.ndarray):
  audio = torch.from_numpy(wav_input.copy()).float()
 elif isinstance(wav_input, Tensor):
  audio = wav_input.float().clone()
 else:
  raise TypeError(f"Unsupported input type for VAD: {type(wav_input)}. Expected np.ndarray or Tensor.")
 if audio.ndim > 1:
  audio = audio.flatten()

 # –ò—Å–ø–æ–ª—å–∑—É–µ–º VAD –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ä–µ—á–∏
 pipeline = get_pipeline()
 sad_segments = pipeline({"waveform": audio.unsqueeze(0), "sample_rate": sr})

 segments: List[torch.Tensor] = []
 curr_duration = 0.0
 curr_start = 0.0
 curr_end = 0.0
 boundaries: List[Tuple[float, float]] = []

 def _update_segments(curr_start: float, curr_end: float, curr_duration: float):
  """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π"""
  if curr_duration > strict_limit_duration:
   # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
   max_segments = int(curr_duration / strict_limit_duration) + 1
   segment_duration = curr_duration / max_segments
   curr_end = curr_start + segment_duration

   for _ in range(max_segments - 1):
    segments.append(audio[int(curr_start * sr): int(curr_end * sr)])
    boundaries.append((curr_start, curr_end))
    curr_start = curr_end
    curr_end += segment_duration

  segments.append(audio[int(curr_start * sr): int(curr_end * sr)])
  boundaries.append((curr_start, curr_end))

 # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
 for segment in sad_segments.get_timeline().support():
  start = max(0, segment.start)
  end = min(audio.shape[0] / sr, segment.end)

  if curr_duration > new_chunk_threshold and (curr_duration + (end - curr_end) > max_duration or curr_duration > min_duration):
   _update_segments(curr_start, curr_end, curr_duration)
   curr_start = start

  curr_end = end
  curr_duration = curr_end - curr_start

 # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
 if curr_duration > new_chunk_threshold:
  _update_segments(curr_start, curr_end, curr_duration)
 return segments, boundaries

def infer_onnx(wav_input: Union[str, np.ndarray, Tensor], model_cfg: omegaconf.DictConfig,
               sessions: List[rt.InferenceSession], preprocessor: Optional[FeatureExtractor] = None,
               tokenizer: Optional[Tokenizer] = None, sample_rate: int = 16000) -> Union[str, np.ndarray]:
 """–ò–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ ONNX Runtime (–±—ã—Å—Ç—Ä–µ–µ –¥–ª—è CPU)"""
 model_name = model_cfg.model_name

 if preprocessor is None:
  preprocessor = FeatureExtractor(  sample_rate=16000,
   features=model_cfg.preprocessor.features  )

 if tokenizer is None and ("ctc" in model_name or "rnnt" in model_name):
  tokenizer = Tokenizer(   model_cfg.decoding.vocabulary,
   model_cfg.decoding.get("model_path")  )

 input_signal = load_audio(wav_input, sample_rate=sample_rate)
 input_signal = preprocessor(input_signal.unsqueeze(0), torch.tensor([input_signal.shape[-1]]))[0].numpy()

 # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å —ç–Ω–∫–æ–¥–µ—Ä–∞
 enc_sess = sessions[0]
 enc_inputs = {node.name: data for (node, data) in zip(
  enc_sess.get_inputs(),
  [input_signal.astype(DTYPE), [input_signal.shape[-1]]]
 )}
 enc_features = enc_sess.run([node.name for node in enc_sess.get_outputs()], enc_inputs)[0]

 # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —ç–º–æ–∫—Ü–∏–π –∏–ª–∏ SSL
 if "emo" in model_name or "ssl" in model_name:
  return enc_features

 blank_idx = len(tokenizer)
 token_ids = []
 prev_token = blank_idx

 # CTC –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
 if "ctc" in model_name:
  prev_tok = blank_idx
  for tok in enc_features.argmax(-1).squeeze().tolist():
   if (tok != prev_tok or prev_tok == blank_idx) and tok != blank_idx:
    token_ids.append(tok)
   prev_tok = tok
 # RNN-T –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
 else:
  pred_states = [
   np.zeros(shape=(1, 1, model_cfg.head.decoder.pred_hidden), dtype=DTYPE),
   np.zeros(shape=(1, 1, model_cfg.head.decoder.pred_hidden), dtype=DTYPE)
  ]
  pred_sess, joint_sess = sessions[1:]

  for j in range(enc_features.shape[-1]):
   emitted_letters = 0
   while emitted_letters < MAX_LETTERS_PER_FRAME:
    pred_inputs = {node.name: data for (node, data) in zip(
     pred_sess.get_inputs(),
     [np.array([[prev_token]])] + pred_states
    )}
    pred_outputs = pred_sess.run([node.name for node in pred_sess.get_outputs()], pred_inputs)

    joint_inputs = {node.name: data for node, data in zip(
     joint_sess.get_inputs(),
     [enc_features[:, :, [j]], pred_outputs[0].swapaxes(1, 2)]
    )}
    log_probs = joint_sess.run([node.name for node in joint_sess.get_outputs()], joint_inputs)

    token = log_probs[0].argmax(-1)[0][0]
    if token != blank_idx:
     prev_token = int(token)
     pred_states = pred_outputs[1:]
     token_ids.append(int(token))
     emitted_letters += 1
    else:
     break
 return tokenizer.decode(token_ids)

def load_onnx(onnx_dir: str, model_version: str) -> Tuple[List[rt.InferenceSession], Union[omegaconf.DictConfig, omegaconf.ListConfig]]:
 """–ó–∞–≥—Ä—É–∂–∞–µ—Ç ONNX –º–æ–¥–µ–ª—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ CPU –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
 opts = rt.SessionOptions()
 opts.intra_op_num_threads = 16  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤
 opts.execution_mode = rt.ExecutionMode.ORT_SEQUENTIAL
 opts.log_severity_level = 3  # –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏

 model_cfg = omegaconf.OmegaConf.load(f"{onnx_dir}/{model_version}.yaml")

 # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
 if "rnnt" not in model_version and "ssl" not in model_version:
  model_path = f"{onnx_dir}/{model_version}.onnx"
  sessions = [rt.InferenceSession(model_path, providers=["CPUExecutionProvider"], sess_options=opts)]
 elif "ssl" in model_version:
  pth = f"{onnx_dir}/{model_version}"
  enc_sess = rt.InferenceSession(f"{pth}_encoder.onnx", providers=["CPUExecutionProvider"], sess_options=opts)
  sessions = [enc_sess]
 else:
  pth = f"{onnx_dir}/{model_version}"
  enc_sess = rt.InferenceSession(f"{pth}_encoder.onnx", providers=["CPUExecutionProvider"], sess_options=opts)
  pred_sess = rt.InferenceSession(f"{pth}_decoder.onnx", providers=["CPUExecutionProvider"], sess_options=opts)
  joint_sess = rt.InferenceSession(f"{pth}_joint.onnx", providers=["CPUExecutionProvider"], sess_options=opts)
  sessions = [enc_sess, pred_sess, joint_sess]

 return sessions, model_cfg

def rtt_half(x: Tensor) -> Tensor:
 """Rotary –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª–æ–≤–∏–Ω—ã –≤–µ–∫—Ç–æ—Ä–∞"""
 x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
 return torch.cat([-x2, x1], dim=x1.ndim - 1)

def apply_rotary_pos_emb(q: Tensor, k: Tensor, cos: Tensor, sin: Tensor, offset: int = 0) -> Tuple[Tensor, Tensor]:
 """–ü—Ä–∏–º–µ–Ω—è–µ—Ç rotary –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
 cos, sin = (cos[offset: q.shape[0] + offset, ...], sin[offset: q.shape[0] + offset, ...])
 return (q * cos) + (rtt_half(q) * sin), (k * cos) + (rtt_half(k) * sin)

class GigaAM(nn.Module):
 """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –º–æ–¥–µ–ª–µ–π GigaAM"""

 def __init__(self, cfg: omegaconf.DictConfig):
  super().__init__()
  self.cfg = cfg

  # –ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ hydra
  self.preprocessor = FeatureExtractor(
   sample_rate=cfg.preprocessor.sample_rate,
   features=cfg.preprocessor.features,
   hop_length=cfg.preprocessor.get("hop_length"),
   win_length=cfg.preprocessor.get("win_length"),
   n_fft=cfg.preprocessor.get("n_fft"),
   center=cfg.preprocessor.get("center", True)
  )

  self.encoder = ConformerEncoder(**cfg.encoder)

 def forward(self, features: Tensor, feature_lengths: Tensor) -> Tuple[Tensor, Tensor]:
  """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏"""
  features, feature_lengths = self.preprocessor(features, feature_lengths)
  return self.encoder(features, feature_lengths)

 def prepare_wav(self, wav_input: Union[str, np.ndarray, Tensor], sample_rate: int = SAMPLE_RATE) -> Tuple[Tensor, Tensor]:
  """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∞—É–¥–∏–æ –¥–ª—è –º–æ–¥–µ–ª–∏"""
  if isinstance(wav_input, str):
   wav = load_audio(wav_input)
   current_sr = SAMPLE_RATE
  elif isinstance(wav_input, np.ndarray):
   if wav_input.size == 0:
    raise ValueError("Audio array is empty")
   wav = torch.from_numpy(wav_input.copy()).float()
   current_sr = sample_rate
   if wav.ndim > 1:
    wav = wav.flatten()
   max_val = wav.abs().max()
   if max_val > 0:
    if max_val > 32768.0:
     wav = wav / 32768.0
    elif max_val > 1.0:
     wav = wav / max_val
  elif isinstance(wav_input, Tensor):
   if wav_input.numel() == 0:
    raise ValueError("Audio tensor is empty")
   wav = wav_input.float().clone()
   current_sr = sample_rate
   if wav.ndim > 1:
    wav = wav.flatten()
   max_val = wav.abs().max()
   if max_val > 0:
    if max_val > 32768.0:
     wav = wav / 32768.0
    elif max_val > 1.0:
     wav = wav / max_val
  else:
   raise TypeError(f"Unsupported input type: {type(wav_input)}. Expected str, np.ndarray, or Tensor.")

  # –†–µ—Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
  if not isinstance(wav_input, str) and current_sr != SAMPLE_RATE:
   if wav.numel() > 0:
    wav = taF.resample(wav.unsqueeze(0), orig_freq=current_sr, new_freq=SAMPLE_RATE).squeeze(0)

  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
  min_length = int(SAMPLE_RATE * 0.1)
  if wav.numel() < min_length:
   raise ValueError(f"Audio is too short: {wav.numel()} samples. Minimum required: {min_length} samples (~0.1 seconds at {SAMPLE_RATE}Hz)")

  wav = wav.unsqueeze(0)
  length = torch.full([1], wav.shape[-1])
  return wav, length

class GigaAMASR(GigaAM):# –ú–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏"""
 def __init__(self, cfg: omegaconf.DictConfig):
  super().__init__(cfg)

  # –ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–æ–ª–æ–≤—ã
  if "ctc" in cfg.model_name:
   self.head = CTCHead(cfg.head.feat_in, cfg.head.num_classes)
   self.decoding = CTCGreedyDecoding(cfg.decoding.vocabulary, cfg.decoding.get("model_path"))
  else:
   self.head = RNNTHead(cfg.head.decoder, cfg.head.joint)
   self.decoding = RNNTGreedyDecoding(cfg.decoding.vocabulary, cfg.decoding.get("model_path"))

 @torch.inference_mode()
 def transcribe(self, wav_input: Union[str, np.ndarray, Tensor], sample_rate: int = SAMPLE_RATE, **kwargs) -> Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]:
  """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç"""
  if isinstance(wav_input, str):
   audio_data = load_audio(wav_input, sample_rate=sample_rate)
  elif isinstance(wav_input, np.ndarray):
   audio_data = torch.from_numpy(wav_input.copy()).float()
   if audio_data.ndim > 1:
    audio_data = audio_data.flatten()
  elif isinstance(wav_input, Tensor):
   audio_data = wav_input.float().clone()
   if audio_data.ndim > 1:
    audio_data = audio_data.flatten()
  else:
   raise TypeError(f"Unsupported input type: {type(wav_input)}. Expected str, np.ndarray, or Tensor.")

  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∞—É–¥–∏–æ
  audio_length = audio_data.shape[-1]

  # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞—É–¥–∏–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–∏–∫–æ–º
  if audio_length <= LONGFORM_THRESHOLD:
   wav, length = self.prepare_wav(wav_input, sample_rate=sample_rate)
   encoded, encoded_len = self.forward(wav, length)
   return self.decoding.decode(self.head, encoded, encoded_len)[0]
  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º
  else:
   print("long")
   transcribed_segments = []
   segments, boundaries = segment_audio_file(audio_data, SAMPLE_RATE, **kwargs)

   for segment, segment_boundaries in zip(segments, boundaries):
    wav = segment.unsqueeze(0)
    length = torch.full([1], wav.shape[-1])
    encoded, encoded_len = self.forward(wav, length)
    result = self.decoding.decode(self.head, encoded, encoded_len)[0]
    transcribed_segments.append({
     "transcription": result,
     "boundaries": segment_boundaries
    })

   return transcribed_segments

def _download_file(file_url: str, file_path: str, force: bool = False) -> str:
 # –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
 if os.path.exists(file_path) and not force:
  return file_path

 os.makedirs(os.path.dirname(file_path), exist_ok=True)

 with urllib.request.urlopen(file_url) as source, open(file_path, "wb") as output:
  with tqdm(total=int(source.info().get("Content-Length", 0)),
            ncols=80, unit="iB", unit_scale=True, unit_divisor=1024) as loop:
   while True:
    buffer = source.read(8192)
    if not buffer:
     break
    output.write(buffer)
    loop.update(len(buffer))
 return file_path

def _download_model(model_name: str, download_root: str, force: bool = False) -> Tuple[str, str]:
 """–°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
 short_names = ["ctc", "rnnt", "e2e_ctc", "e2e_rnnt", "ssl"]
 possible_names = short_names + list(_MODEL_HASHES.keys())

 if model_name not in possible_names:
  raise ValueError(f"Model '{model_name}' not found. Available model names: {possible_names}")

 if model_name in short_names:
  model_name = f"v3_{model_name}"

 model_url = f"{_URL_DIR}/{model_name}.ckpt"
 model_path = os.path.join(download_root, f"{model_name}.ckpt")

 if os.path.exists(model_path) and not force:
  logging.info(f"Model found at {model_path}, skipping download.")
 else:
  logging.info(f"Downloading model to {model_path}")
 return model_name, _download_file(model_url, model_path, force)

def _download_tokenizer(model_name: str, download_root: str, force: bool = False) -> Optional[str]:
 # –°–∫–∞—á–∏–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
 if model_name != "v1_rnnt" and "e2e" not in model_name:
  return None
 tokenizer_url = f"{_URL_DIR}/{model_name}_tokenizer.model"
 tokenizer_path = os.path.join(download_root, f"{model_name}_tokenizer.model")

 if os.path.exists(tokenizer_path) and not force:
  logging.info(f"Tokenizer found at {tokenizer_path}, skipping download.")
 else:
  logging.info(f"Downloading tokenizer to {tokenizer_path}")

 return _download_file(tokenizer_url, tokenizer_path, force)

def check_model_exists(model_name: str, download_root: str) -> bool:
 # –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å"""
 short_names = ["ctc", "rnnt", "e2e_ctc", "e2e_rnnt", "ssl"]
 if model_name in short_names:
  model_name = f"v3_{model_name}"

 model_path = os.path.join(download_root, f"{model_name}.ckpt")
 if not os.path.exists(model_path):
  return False

 # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö—ç—à –µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ
 expected_hash = _MODEL_HASHES.get(model_name)
 if expected_hash:
  actual_hash = hashlib.md5(open(model_path, "rb").read()).hexdigest()
  if actual_hash != expected_hash:
   logging.warning(f"Model exists but hash mismatch: expected {expected_hash}, got {actual_hash}")
   return False
 return True

def _normalize_device(device: Optional[Union[str, torch.device]]) -> torch.device:
 """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ - –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç CPU"""
 return torch.device("cpu")

class GigaAMEmo(GigaAM):# –ú–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π"""
 def __init__(self, cfg: omegaconf.DictConfig):
  super().__init__(cfg)
  self.head = nn.Linear(cfg.head.in_features, cfg.head.out_features)
  self.id2name = cfg.id2name

 def get_probs(self, wav_input: Union[str, np.ndarray, Tensor], sample_rate: int = SAMPLE_RATE) -> Dict[str, float]:
  """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —ç–º–æ—Ü–∏–∏ –≤ –∞—É–¥–∏–æ"""
  wav, length = self.prepare_wav(wav_input, sample_rate=sample_rate)
  encoded, _ = self.forward(wav, length)
  # –ü—É–ª–∏–Ω–≥ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
  encoded_pooled = nn.functional.avg_pool1d(encoded, kernel_size=encoded.shape[-1]).squeeze(-1)
  logits = self.head(encoded_pooled)[0]
  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —ç–º–æ—Ü–∏–π
  probs = nn.functional.softmax(logits, dim=-1).detach().tolist()
  return {self.id2name[i]: probs[i] for i in range(len(self.id2name))}

def load_model( model_name: str, download_root: Optional[str] = None,  # 4. –ö–æ—Ä–Ω–µ–≤–æ–π –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
  force_download: bool = False,  # 5. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
  fp16_encoder: bool = True,  # 1. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è, —Ç.–∫. FP16 –ø–æ–ª–µ–∑–µ–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ GPU
  use_flash: Optional[bool] = False,  # 2. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è, —Ç.–∫. FlashAttention –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ CPU
  device: Optional[Union[str, torch.device]] = None  ) -> Union[GigaAM, GigaAMEmo, GigaAMASR]: # 3. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è, –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ CPU
 """ –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è CPU
 Args:
     model_name: –ò–º—è –º–æ–¥–µ–ª–∏ (ctc, rnnt, emo, ssl –∏ —Ç.–¥.)
     download_root: –ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
     fp16_encoder: –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (—Ç–æ–ª—å–∫–æ –¥–ª—è GPU)
     use_flash: –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (FlashAttention –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ CPU)
     device: –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (–º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ CPU)
     force_download: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å

 Returns:
     –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –∏ –≥–æ—Ç–æ–≤–∞—è –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª—å
 """
 if download_root is None:
  raise ValueError("download_root must be specified")
 os.makedirs(download_root, exist_ok=True)

 device_obj = torch.device("cpu") # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

 # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
 if not check_model_exists(model_name, download_root) or force_download:
  _download_model(model_name, download_root, force_download)

 model_name, model_path = _download_model(model_name, download_root, force_download)
 tokenizer_path = _download_tokenizer(model_name, download_root, force_download)

 # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö—ç—à –º–æ–¥–µ–ª–∏
 if os.path.exists(model_path):
  actual_hash = hashlib.md5(open(model_path, "rb").read()).hexdigest()
  expected_hash = _MODEL_HASHES.get(model_name)
  if expected_hash and actual_hash != expected_hash:
   if force_download:
    logging.warning(f"Model hash mismatch: expected {expected_hash}, got {actual_hash}. Re-downloading...")
    os.remove(model_path)
    model_name, model_path = _download_model(model_name, download_root, True)
   else:
    raise RuntimeError(
     f"Model checksum failed for {model_name}. "
     f"Expected {expected_hash}, got {actual_hash}. "
     f"Please delete {model_path} and reload the model, "
     f"or use force_download=True to re-download automatically."  )

 if not os.path.exists(model_path):
  raise FileNotFoundError(f"Model file not found: {model_path}. Please check the download directory.")

 # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
 with warnings.catch_warnings():
  warnings.simplefilter("ignore", category=(FutureWarning))
  checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)

 # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è CPU
 cfg = checkpoint["cfg"]

 # –û—Ç–∫–ª—é—á–∞–µ–º GPU-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
 if "encoder" in cfg:
  cfg.encoder.flash_attn = False  # FlashAttention –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ CPU
  cfg.encoder.fp16 = False  # FP16 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ CPU

 # –ó–∞–º–µ–Ω—è–µ–º hydra –Ω–∞ –ø—Ä—è–º—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
 if "preprocessor" in cfg:
  cfg.preprocessor["_target_"] = "sber_gegaam.FeatureExtractor"

 if "encoder" in cfg:
  cfg.encoder["_target_"] = "sber_gegaam.ConformerEncoder"

 # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
 if tokenizer_path is not None and "decoding" in cfg:
  cfg.decoding.model_path = tokenizer_path

 # –°–æ–∑–¥–∞–µ–º –Ω—É–∂–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏
 if "ssl" in model_name:
  model = GigaAM(cfg)
 elif "emo" in model_name:
  model = GigaAMEmo(cfg)
 else:
  model = GigaAMASR(cfg)

 # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
 model.load_state_dict(checkpoint["state_dict"], strict=True)
 model.eval()

 # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è CPU
 torch.set_num_threads(min(8, os.cpu_count() or 8))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

 # –û—Ç–∫–ª—é—á–∞–µ–º autograd –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
 for param in model.parameters():
  param.requires_grad = False
 try: # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (PyTorch 2.0+)
  if hasattr(torch, 'compile'):
   model = torch.compile(model, mode="reduce-overhead")
 except:
  pass  # –ï—Å–ª–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, —Ä–∞–±–æ—Ç–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
 cfg.model_name = model_name
 return model.to(device_obj)


# –≠–∫—Å–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
__all__ = ["GigaAM", "GigaAMASR", "GigaAMEmo", "load_audio", "load_model"]